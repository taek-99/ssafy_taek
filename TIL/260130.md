# 데이터엔지니어링

## 챕터
- DataFame과 SparkSQL
- DSL과 SQL을 활용한 데이터 처리

## Remind
- RDD란
  - 데이터를 병렬 처리하는 핵심적인 역할을 수행하여 빠르고 안정적으로 동작하는 프로그램을 작성 가능
  - 데이터 값 자체는 표현이 가능하지만, 데이터에 대한 메타 데이터, '스키마'에 대해 명시적 표현 방법이 없음

- RDD API의 문제점
  - 스파크가 RDD API 기반의 연산, 표현식을 검사하지 못해 최적화할 방법이 없음
  - 스파크는 어떠한 데이터 압축 테크닉도 적용하지 못함
  
## DataFrame
- 스키마를 가진 분산 데이터 컬렉션
- 데이터를 행과 열로 구성된 표 형태로 관리
- 각 열은 명확한 데이터 타입과 메타 데이터를 가지고 있음
- 스파크SQL이 제공하는 구조화된 데이터 모델로서 RDD의 한계를 보완

## Dataframe API - 개요
- 구조, 포멧 등 몇몇 특정 연산 등에 있어, Pandas의 Dataframe에 영향을 많이 받음
- 이름 있는 컬럼과 스키마를 가진 분산 인메모리 테이블처럼 동작

- 데이터 타입
  - 실제 데이터를 위한 스키마를 정의할 때 어떻게 이런 타입들이 연계되는지를 아는 것이 중요

## Dataframe API - 스키마
- 스파크에서의 스키마는 Dataframe을 위해 컬럼 이름과 연관된 데이터 타입을 정의한 것
- 외부 데이터 소스에서 구조화된 데이터를 읽어 들일 때 사용
- 읽을때 스키마를 가져오는 방식과 달리, 미리 스키마를 정의하는 것은 여러 장점 존재

## SparkSQL과 DF 소개

### RDD와 DataFrame의 차이점
- 데이터 표현 방식
  - RDD: 값만 표현 가능, 스키마 표현 불가능
  - DF: 명확한 스키마를 가진 구조적 데이터

- 최적화 및 성능
  - RDD: 최적화하기 어려움, 직접적 연산 필요
  - DF: Catalyst Optimizer를 통한 자동 최적화 및 빠른 처리가능

- 사용 편의성
  - RDD: 낮음(저수준 API)
  - DF: 높음(고수준 API, SQL 활용 가능)

- DF를 사용하면 데이터를 더욱 효율적이고 편리하게 처리할 수 있으며, 데이터의 메타 정보를 활용하여 더 빠르고 최적화된 분석을 수행할 수 있다

### RDD를 사용하는 경우
- 저수준의 트랜스포메이션과 액션을 직접 제어해야 할때
- 스트림 데이터가 구조화되지 않은 경우
- 특정 도메인 표현을 위해 함수형 프로그래밍이 필요할때
- 스키마 변환이 필요 없을 때
- DF나 Dataset에서 처리할 수 없는 성능 최적화가 필요할때

### DataFrame을 사용하는 경우
- 고수준의 추상화와 도메인 기반 API가 필요할때
- 고수준의 표현 등 복잡한 연산이 필요하거나 반구조적 데이터에 대한 람다식이 필요할때
- 타입 안정성과 최적화를 위해 컴파일 시 타입 안정성 보장
- Catalyst 최적화 및 Tungsten의 효율적인 코드 제너레이션이 필요할때
- Spark API의 일관성과 간결함을 원할때

### SparkSQL이란
- SparkSQL은 구조화된 데이터를 SQL처럼 처리할 수 있도록 해주는 스파크 모듈
- 내부적으로는 DataFrame/Dataset API와 동일한 엔진을 사용하여 처리
- DataFrame과 Dataset을 SQL처럼 다룰 수 있게 해주는 분산 SQL 쿼리 엔진
- Spark SQL은 RDD보다 더 높은 수준의 추상화와 자동 최적화를 제공

### SparkSQL의 역할
- SQL같은 질의 수행
- 스파크 컴포넌트들을 통합하고, Dataframe, Dataset가 java, scala, python, R 등 여러 프로그래밍 언어로 정형화 데이터 관련 작업을 단순화할 수 있도록 추상화


## Spark SQL

### SQL쿼리 기본 문법
- 데이터 조회: SELECT, WHERE
- 정렬: ORDER BY
- 중복 제거: DISTINCT
- 데이터 집계: GROUP BY, HAVING, 집계 함수(COUNT, AVG, SUM)
- 데이터 결합: JOIN


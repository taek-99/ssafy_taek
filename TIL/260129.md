# 데이터엔지니어링

## 챕터
- RDD의 개념과 특징
- RDD 생성 및 변환

## RDD의 개념과 특징
- 대용량 데이터를 분산 처리하고 분석하기 위한 Spark의 기본 데이터 처리 단위
- resilient 탄력적인
- distributed 분산된
- dataset 데이터셋

### RDD 특징
- 탄력성 & 불변성
  - RDD는 한번 생성되면 변경할 수 없음
  - 어떤 노드가 장애로 인해 중단되더라도, 데이터 복구 가능

- 타입의 안정성 보장
  - 어떠한 하나의 타입의 객체를 가질 수 있음
  - 데이터 타입을 컴파일 시전에 검사
  - 성능 최적화

- 정형&비정형 데이터
  - 비정형 데이터: 고정된 포맷이 없는 텍스트 데이터
    - sc.textFil()을 이용해 RDD로 로딩 후 map, filter, flatMap 등으로 가공
  - 정형 데이터: 컬럼이 있는 테이블 형태 데이터
    - DataFrame 또는 RDD.map()으로 가공

- 지연 평가
  - 중간 연산을 줄여 성능 최적화
  - 실행 계획을 최적화하여 성능 향상

### RDD 생성
- 기존의 메모리 데이터를 RDD로 변환하는법
  - 파이썬의 리스트나 Scala의 컬랙션을 RDD로 변호나 가능
  - 주로 테스트나 작은 데이터 셋을 다룰 때 사용
  
- 외부파일 에서 RDD를 생성하는 법
  - 실무에서는 보통 파일이나 데이터베이스에서 데이터를 불러와야함
  - sc.textFile, spark.read.format.option형태를 사용하여 외부 데이터를 RDD로 변환할 수 있음


### RDD생성: 메모리 데이터 활용
- 기존 메모리 데이터를 Spark의 RDD로 변환하는 역할
  - sc.parallelize
- parallelize()는 메모리에 있는 데이터를 Spark 클러스터로 보낼 때 사용

### RDD생성: 외부 파일에서 데이터 읽기
- 외부 파일에서 데이터를 직접 읽어와 RDD로 변환하는 역할
  - sc.textFile()

### RDD변환
- MAP
  - x.map(lambda z: (z, 1))

- FLATMAP
  - x.flatMap(lambda x: (1*x, 2*x, 3*x, 100))

- FILTER
  - x.filter(lambda x: x%2 == 1)

- MAPPARTITONS
  - x.mapPartitions(f)

- MAPPARTITIONS WITH INDEX
  - x.mapPartitionsWithIndex(f)

- KEYBY
  - x.keyby(lambda w:w[0])

- GROUPBY
  - x.groupBy(lambda w: w[0])

- GROUPBYKEY
  - x.groupByKey()

- REDUCEBYKEY
  - GROUPBYKEY보다 효율 좋음

- 파티셔닝 재분배 (Repartition vs Coalesce)
  - Repartition 늘리는거
  - Coalesce 줄이는거

- ACTIONS
  - collect: 모든 데이터를 리스트로 변환
  - count: 전체 요소를 하나로 결합
  - reduce: 전체 요소를 하나로 결합
  - sum: 요소의 합 반환
  - mean: 평균 값 반환
# 데이터 분석을 위한 머신러닝 알고리즘2

## 챕터
- SVM
- 앙상블
- 군집
- Bias, Variance, Regularizations

## SVM
- 서포트 백터머신은 여백을 최대화하는 초평면을 찾는 지도 학습 알고리즘


### 서포트 백터 머신이란?
- 데이터를 고차원 공간에 매핑하여 두 클래스를 최대한 잘 구분할 수 있는 최적의 초평면을 찾는 분류 알고리즘

### 작동방식
- 초평면 결정: 두 클래스 사이의 간격을 최대화하는 초평면을 찾음
- 서포트 벡터: 초평면에 가장 가까운 데이터 포인트들이 서포트 벡터 역할을 하며, 이들이 결정 경계를 형성
- 비선형 데이터 처리: 커널 트릭을 사용하여 비선형 데이터를 고차원 공간으로 변환한 후, 선형 분리를 수행할 수 있음

### 서포트 벡터 머신
- 장점
    - 상대적으로 데이터의 이해도가 떨어져도 사용이 용이함
    - 예측 정확도가 통상적으로 높음
- 단점
    - C(에러를 부여하는 가중치)를 결정해야함
    - 파라미터의 결정과 모형의 구축에 시간이 오래 걸림
- 여백의 의미
    - 주어진 데이터가 오류를 발생시키지 않고 움직일 수 있는 최대 공간
- 최적의 결졍 경계
    - 최적의 결정 경계는 데이터 군집으로부터 최대한 멀리 떨어지는것
- 분류에서의 여백
    - 데이터가 2차원이며, 라벨은 파란색과 빨간색 두개의 클래스가 있다고 가정
- soft margin vs hard margin
    - soft margin
        - 결정 경계를 조금씩 넘어가는 데이터들을 어느정도 허용하여 유연한 결정 경계를 만듦
        - 어느정도 비용을 감수하면서 최선의 결정 경계를 찾음
    - hard margin
        - 이상치들을 허용하지 않고, 분명하게 나누는 결정 경계를 만듦
        - 과적합의 오류가 발생하기 쉬움

- kernel Trick
    - svm 기본적으로 선형 분류를 위한 결정 경계를 만들지만, Kernel Trick을 사용한다면 비선형 분류도 가능
    - 고차원에서 사용
    - 대표적인 Kernel 함수
        - Linear: 선형 함수
        - Poly: 다항식 함수
        - RBF: 방사 기저 함수
        - Hyperbolicc Tangent: 쌍곡선 탄젠트

## 앙상블
- Tree 기반 모델과 앙상블 기법
    - 의사결정 트리
        - 분류, 회귀 문제에 모두 사용할 수 있는 모델
        - 입력변수를 특정한 기준으로 분기해 트리 형태의 구조로 분류하는 모델
    - 장점
        - 해석이 쉬움
        - 입력 값이 주어졌을 때 설명 변수의 영역의 흐름을 따라 출력값이 어떻게 나오는지 파악하기 용이
    - 단점
        - 예측력이 떨어짐
        - 단순히 평균 또는 다수결 법칙에 의해 예측을 수행

- 앙상블 학습
    - 여러개의 분류기를 생성하고, 그 예측을 결합함으로써 보다 정확한 예측을 도출하는 기법
    - 강력한 한의 모델 대신 약한 모델 여러개를 조합하여 더 정확한 예측에 도움을 주는 방식
    - 보팅, 배깅, 부스팅 세가지의 유형이 존재

- 보팅
    - 여러개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식
    - 하드 보팅: 다수의 분류기가 예측한 결과값을 최종 결과로 선정
    - 소프트 보팅: 모든 분류기가 예측한 레이블값의 결정 확률 평균을 구한 뒤 가장 확률이 높은 레이블 값을 최종 결과로 선정

- 배깅
    - 데이터 샘플링을 통해 모델을 학습시키고 결과를 집계하는 방법으로 모두 같은 유형의 알고리즘 기반의 분류기를 사용
    - 과적합 방지에 효과적

- 부스팅
    - 여러개의 분류기가 순차적으로 학습을 수행
    - 이전 분류기가 예측이 틀린 데이터에 대해 올바르게 예측할 수 있도록 다음 분류기에게 가중치를 부여하면서 학습과 예측을 진행

- 랜덤 포레스트
    - 여러 개의 결정 트리들을 임의적으로 학습하는 방식의 앙상블 방법
    - 기반 기술
        - 의사결정 트리: 여러가지 요소를 기준으로 갈라지는 가지를 트리 형태로 구성하여 분석하는 기법
        - 앙상블 트리의 한계
            - 학습 데이터에 따라 생성되는 결정 트리가 크게 다랄져 일반화가 어려운 과적합 문제 발생
    - 특징
        - 임의성: 서로 조금씩 다른 특성의 트리들로 구성
        - 비상관화: 각 트리들의 예측이 서로 연관되지 않음
        - 견고성: 오류가 전파되지 않아 노이즈에 강함
        - 일반화: 임의화를 통한 과적합 문제 극복

    - 장점
        - 과적합이 잘 일어나지 ㅇ낳음
        - 결측치나 이상치에 강함
        - 비선형 데이터에 강함
    - 단점
        - 수많은 트리를 계산하기 때무에 학습 시간과 계산 연산량이 큼


## 군집
- 군집 분석은 비슷한 특성을 가진 데이터를 그룹화하는 비지도 학습 기법
- 데이터의 내재된 구조를 발견하고, 패턴을 파악하는데 활용
- 클러스터간 거리 계산 방법
    - 단일 연결법
        - 각각의 군집에서 하나씩 선택한 개체를 연결했을때 가장 짧은 거리
    - 완전 연결법
        - 각각의 군집에서 하나씩 선택한 개체를 연결했을때 가장 긴거리
    - 평균 연결법
        - 각각의 군집에서 하나식 선책한 개체를 연결한 전체 거리의 평균
    - 중심 연결법
        - 각각읜 군집 내에서 중간에 해당하는 점을 연결한 거리
    - ward 연결법
        - 각각의 군집 내에서 개체의 중간에 해당하는 점과 각각의 개체 걍 ㅈㄴ 복잡함

- K-평균
    - K-평균 알고리즘은 가장 대표적인 군집화 알고리즘
    - 사용자가 지정한 K개의 군집 중심점을 초기화하고, 반복적으로 업데이트
    - 각 데이터 포인트를 가장 가까운 군집 중심점에 할당하고 군집 중심점을 재계산하는 과정을 수렴할때까지 반복
    - 간단하고 빠르지만, K값을 사전에 지정해야함


    

## Bias, Variance, Regularization
- 예측 오차의 구성
    - 편향: 모델이 단순화되어 발생하는 오류
    - 분산: 모델이 데이터의 작은 변화에 민감하게 반응하여 발생하는 오류
    - 불확실성: 데이터 자체의 본질적인 변동성

- Bias(편향)
    - 모델이 실제 데이터 패턴을 단순화하여 학습
    - 편향이 높으면 복잡한 패턴을 제대로 학습하지 못해 언더피팅

- Variance(분산)
    - 모델이 학습 데이터에 민감하여 작은 변화에도 큰 차이를 보임
    - 분산이 높으면 학습데이터에 과적합하여 오버피팅

- Bias-Variance Trade-Off
    - 모델의 복잡도를 조절하면서 편향과 분산간의 균형을 맞추는것
    - 복잡도가 높으면: 편향 Down, 분산 Up, 오버피팅 위험
    - 복잡도가 낮으면: 편향 Up, 분산 Down, 언더피팅 위험
